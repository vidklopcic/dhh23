{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T21:10:41.090727Z",
     "start_time": "2023-05-24T21:10:41.022887Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open('artefacts/parlamint_with_embeddings.json', 'r') as f:\n",
    "    parlamint = json.load(f)\n",
    "parlamint_es_ga = parlamint['ES-GA']\n",
    "parlaming_gb = parlamint['GB']\n",
    "parlamint_hu = parlamint['HU']\n",
    "parlamint_ua = parlamint['UA']\n",
    "parlamint_si = parlamint['SI']\n",
    "parlamints = [parlamint_si, parlaming_gb, parlamint_es_ga, parlamint_hu, parlamint_ua]\n",
    "parlamint_si[0].keys()\n",
    "artefacts = Path('artefacts/tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T20:07:54.069914Z",
     "start_time": "2023-05-24T20:02:51.095469Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ES-GA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [01:26<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing HU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 515/515 [02:00<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing UA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1091/1091 [01:32<00:00, 11.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def score_tf_idf(document):\n",
    "    lemmas = set()\n",
    "    xml_file = document['xml_path']\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "\n",
    "    for word in soup.find_all('w'):\n",
    "        lemma: str = word.get('lemma')\n",
    "        if not lemma or not lemma.isalpha():\n",
    "            continue\n",
    "        lemmas.add(lemma.lower())\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def process_parlamint(parlamint):\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        result_list = []\n",
    "        for result in tqdm(pool.imap_unordered(score_tf_idf, parlamint), total=len(parlamint)):\n",
    "            result_list.append(result)\n",
    "    lemma_frequencies = {}\n",
    "    for lemmas in result_list:\n",
    "        for lemma in lemmas:\n",
    "            lemma_frequencies[lemma] = lemma_frequencies.get(lemma, 0) + 1\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_lemma_frequencies.json', 'w') as f:\n",
    "        json.dump(lemma_frequencies, f)\n",
    "\n",
    "\n",
    "for parlamint in parlamints:\n",
    "    process_parlamint(parlamint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T21:08:02.761033Z",
     "start_time": "2023-05-24T20:52:12.856337Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing SI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1572/1572 [14:50<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_sentence_scores(args):\n",
    "    lemma_frequencies, document_count, xml_file = args\n",
    "    with open(lemma_frequencies, 'r') as f:\n",
    "        lemma_frequencies = json.load(f)\n",
    "\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "\n",
    "    scores: dict[str, dict[str, int]] = {}\n",
    "    speaker_ids = set([i.get('who') for i in soup.find_all('u') if i.get('who')])\n",
    "    for speaker_id in speaker_ids:\n",
    "        u_elements = soup.find_all('u', {'who': speaker_id})\n",
    "        sentences = [u.find_all('s') for u in u_elements]\n",
    "        sentences = [item for sublist in sentences for item in sublist]\n",
    "\n",
    "        speaker_tf = {}\n",
    "        speaker_sentences = {}\n",
    "        for sentence in sentences:\n",
    "            sentence_id = sentence.get('xml:id')\n",
    "            speaker_sentences[sentence_id] = []\n",
    "            for word in sentence.find_all('w'):\n",
    "                lemma: str = word.get('lemma')\n",
    "                if not lemma or not lemma.isalpha():\n",
    "                    continue\n",
    "                lemma = lemma.lower()\n",
    "                speaker_tf[lemma] = speaker_tf.get(lemma, 0) + 1\n",
    "                speaker_sentences[sentence_id].append(lemma)\n",
    "\n",
    "        for sentence_id, sentence in speaker_sentences.items():\n",
    "            scores[sentence_id] = []\n",
    "            for lemma in sentence:\n",
    "                tf = speaker_tf[lemma]\n",
    "                df = lemma_frequencies[lemma]\n",
    "                idf = math.log(document_count / df) if df else 0  # IDF\n",
    "                score = tf * idf  # TF-IDF\n",
    "                scores[sentence_id].append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def process_parlamint(parlamint):\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        sentences = {}\n",
    "        for result in tqdm(pool.imap_unordered(get_sentence_scores, [\n",
    "            (artefacts / f'{i[\"language\"]}_lemma_frequencies.json', len(parlamint), i['xml_path']) for i in parlamint\n",
    "        ]), total=len(parlamint)):\n",
    "            sentences.update(result)\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_scores.json', 'w') as f:\n",
    "        json.dump(sentences, f)\n",
    "\n",
    "for parlamint in parlamints[1:]:\n",
    "    process_parlamint(parlamint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T21:12:09.130564Z",
     "start_time": "2023-05-24T21:10:56.904289Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing SI\n",
      "max_score 540.1589589347477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for parlamint in parlamints[1:]:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_scores.json', 'r') as f:\n",
    "        sentence_scores = json.load(f)\n",
    "    max_score = max([s for ss in sentence_scores.values() for s in ss])\n",
    "    print('max_score', max_score)\n",
    "    sentece_weights = {}\n",
    "    for sentence_id in sentence_scores:\n",
    "        sentece_weights[sentence_id] = np.mean(sentence_scores[sentence_id]) / max_score\n",
    "\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'w') as f:\n",
    "        json.dump(sentece_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T21:40:57.683428Z",
     "start_time": "2023-05-24T21:37:34.219910Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing SI\n",
      "243 ventilatorjev\n",
      "453 ventilatorjev\n",
      "Merilniki slanosti\n",
      "404 ventilatorji 13. marca\n",
      "Prehajamo na podprogram Štipendije\n",
      "Podjetje Lastinski je prišlo po merilnike slanosti k Darsu\n",
      "Multinacionalka je prodala žilno opornico recimo Mark Medicalu S. p.\n",
      "Oklepnike\n",
      "Ceca\n",
      "Prehajamo na podprogram Dolgotrajna oskrba\n",
      "Prehajamo na podprogram Univerzitetne knjižnice\n",
      "Prehajamo na podprogram Kakovost zraka\n",
      "Lastniki so bili Mark Medical S. p.\n",
      "Prva pogodba med Darsom in podjetjem Lastinski\n",
      "To se pravi podjetje Lastinski je vrnilo merilnike slanosti\n",
      "Palestina\n",
      "Palestina\n",
      "Burka\n",
      "Nazaj k ventilatorjem\n",
      "Mark Medical Slovenija je v lastniški družbi KB 1909\n",
      "Tako pride do posla podjetje Lastinski\n",
      "Število ventilatorjev\n",
      "Število ventilatorjev\n",
      "Prehajamo na podprogram Podpora raziskovalni infrastrukturi\n",
      "Prehajamo na podprogram Podpora raziskovalni infrastrukturi\n",
      "Pomožnega motorja\n",
      "Prehajamo na podprogram Kreiranje delovnih mest\n",
      "Prehajamo na podprogram Kreiranje delovnih mest\n",
      "Poglejte kaj je pa firma Lastinski odgovorila Darsu\n",
      "Prehajamo še na podprogram Športna infrastruktura\n",
      "Prehajamo še na podprogram Športna infrastruktura\n",
      "Prehajamo še na podprogram Umetnostni programi\n",
      "Prehajamo na podprogram Izvajanje osnovnošolskih programov\n",
      "Prehajamo na podprogram Izvajanje osnovnošolskih programov\n",
      "Prehajamo še na podprogram Dolgotrajna oskrba\n",
      "Prehajamo na podprogram Dejavnost višjega šolstva\n",
      "Prehajamo na podprogram Dejavnost visokega šolstva\n",
      "Prehajamo na podprogram Dejavnost visokega šolstva\n",
      "Prehajamo na podprogram Programi socialnega varstva\n",
      "Prehajamo na podprogram Upravljanja z vodami\n",
      "Prehajamo na podprogram Upravljanje z vodami\n",
      "Manjkalo je 130 žilnih opornic\n",
      "8. oktobra 2013 lani oktobra torej prejme Dars ponudbo podjetja Lastinski d. o. o. za umerjanje merilnikov slanosti\n",
      "Prehajamo na podprogram Založništvo\n",
      "Odločamo o amandmaju poslanskih skupin LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin LMŠ in SAB k temu podprogramu\n",
      "Še glede podelitve koncesije Termam Čatež\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "--------\n",
      "sejo\n",
      "Državnega zbora\n",
      "Da\n",
      "Da\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Državni zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani Zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def print_ids(ids):\n",
    "    def get_path(id):\n",
    "        year = id.split('_')[1].split('-')[0]\n",
    "        return f'data/annotated/ParlaMint-SI.TEI.ana/{year}/{id.split(\".seg\")[0]}.xml'\n",
    "\n",
    "    soups = {}\n",
    "    for sid in ids:\n",
    "        xml_path = get_path(sid)\n",
    "        if xml_path not in soups:\n",
    "            with open(xml_path, 'r') as f:\n",
    "                contents = f.read()\n",
    "            soups[xml_path] = BeautifulSoup(contents, 'xml')\n",
    "        soup = soups[xml_path]\n",
    "        sentence = soup.find('s', {'xml:id': sid})\n",
    "        print(' '.join([w.get_text() for w in sentence.find_all('w')]))\n",
    "\n",
    "for parlamint in parlamints[:1]:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'r') as f:\n",
    "        sentece_weights = json.load(f)\n",
    "        top_50 = heapq.nlargest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(top_50)\n",
    "        print('--------')\n",
    "        bottom_50 = heapq.nsmallest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(bottom_50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
