{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:44:48.314081Z",
     "start_time": "2023-05-25T14:44:48.248268Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['language', 'date', 'xml_path', 'embeddings_path'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('artefacts/parlamint_with_embeddings.json', 'r') as f:\n",
    "    parlamint = json.load(f)\n",
    "parlamint_es_ga = parlamint['ES-GA']\n",
    "parlaming_gb = parlamint['GB']\n",
    "parlamint_hu = parlamint['HU']\n",
    "parlamint_ua = parlamint['UA']\n",
    "parlamint_si = parlamint['SI']\n",
    "parlamints = [\n",
    "    parlamint_si,\n",
    "    parlamint_es_ga,\n",
    "    parlaming_gb,\n",
    "    parlamint_hu,\n",
    "    parlamint_ua,\n",
    "]\n",
    "artefacts = Path('artefacts/tf_idf')\n",
    "parlamint_si[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create mappings for speech ids to xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:04.062413Z",
     "start_time": "2023-05-25T14:35:58.908905Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                | 0/1572 [00:02<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def worker(document):\n",
    "    rows = []\n",
    "    with open(document['xml_path'], 'r', encoding='utf8') as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "    for speech in soup.find_all('u'):\n",
    "        speech_id = speech.get('xml:id')\n",
    "        rows.append([speech_id, document['xml_path']])\n",
    "    return rows\n",
    "\n",
    "\n",
    "columns = ['speech_id', 'xml_path']\n",
    "all_rows = []\n",
    "\n",
    "# Create a multiprocessing pool\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for parlamint in parlamints:\n",
    "        # Use pool.map or pool.imap to run the worker function in parallel on each document\n",
    "        # tqdm can be used in conjunction with pool.imap for a progress bar\n",
    "        for result in tqdm(pool.imap(worker, parlamint), total=len(parlamint)):\n",
    "            all_rows.extend(result)\n",
    "\n",
    "df = pd.DataFrame(all_rows, columns=columns)\n",
    "df.to_feather('artefacts/mappings/speech2xml.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Parameters\n",
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:05.562974Z",
     "start_time": "2023-05-25T14:36:05.562688Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TFIDFJob:\n",
    "    tf_idf_output_dir: Path\n",
    "    speech_cluster_labels: list[str]\n",
    "    speech_clusters: list[set[str]]\n",
    "    corpus_documents: Optional[Any] = None\n",
    "    mp_corpus_documents: Optional[Any] = None\n",
    "\n",
    "    @classmethod\n",
    "    def pickle_path(cls, tf_idf_output_dir: Path):\n",
    "        return tf_idf_output_dir / 'job.pickle'\n",
    "\n",
    "    def to_pickle(self):\n",
    "        with open(self.pickle_path(self.tf_idf_output_dir), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'tf_idf_output_dir': self.tf_idf_output_dir,\n",
    "                'speech_cluster_labels': self.speech_cluster_labels,\n",
    "                'speech_clusters': self.speech_clusters,\n",
    "            }, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pickle(cls, tf_idf_output_dir: Path):\n",
    "        pickle_path = cls.pickle_path(tf_idf_output_dir)\n",
    "        if not pickle_path.exists():\n",
    "            return None\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "def discover_speech_ids(xml_documents: list[str]):\n",
    "    speech_ids = set()\n",
    "    for xml_file in xml_documents:\n",
    "        with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "            contents = f.read()\n",
    "        soup = BeautifulSoup(contents, 'xml')\n",
    "        for speech in soup.find_all('u'):\n",
    "            speech_id = speech.get('xml:id')\n",
    "            speech_ids.add(speech_id)\n",
    "    return speech_ids\n",
    "\n",
    "\n",
    "def get_speech_ids_by_speaker_worker(xml_file):\n",
    "    speech_ids_by_speaker = defaultdict(set)\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "    for speech in soup.find_all('u'):\n",
    "        speech_id = speech.get('xml:id')\n",
    "        speaker = speech.get('who')\n",
    "        speech_ids_by_speaker[speaker].add(speech_id)\n",
    "    return dict(speech_ids_by_speaker)\n",
    "\n",
    "\n",
    "def get_speech_ids_by_speaker(xml_documents: list[str]):\n",
    "    speech_ids_by_speaker = defaultdict(set)\n",
    "\n",
    "    # Create a multiprocessing pool\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        # Use pool.imap to run the worker function in parallel on each xml_file\n",
    "        # tqdm can be used in conjunction with pool.imap for a progress bar\n",
    "        for result in tqdm(pool.imap(get_speech_ids_by_speaker_worker, xml_documents), total=len(xml_documents)):\n",
    "            # Merge the result into the final dictionary\n",
    "            for speaker, speech_ids in result.items():\n",
    "                speech_ids_by_speaker[speaker].update(speech_ids)\n",
    "\n",
    "    return dict(speech_ids_by_speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TF-IDF per speaker for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_jobs = []\n",
    "for parlamint in parlamints:\n",
    "    print('discovering speech ids by speaker for', parlamint[0]['language'])\n",
    "    speech_ids_by_speaker = get_speech_ids_by_speaker([d['xml_path'] for d in parlamint])\n",
    "    speech_ids_by_speaker = list(speech_ids_by_speaker.items())\n",
    "    tf_idf_output_dir = artefacts / 'by_speaker' / parlamint[0][\"language\"]\n",
    "    job = TFIDFJob.from_pickle(tf_idf_output_dir)\n",
    "    if job:\n",
    "        tf_idf_jobs.append(job)\n",
    "        continue\n",
    "    tf_idf_jobs.append(\n",
    "        TFIDFJob(\n",
    "            tf_idf_output_dir=tf_idf_output_dir,\n",
    "            speech_cluster_labels=[i[0] for i in speech_ids_by_speaker],\n",
    "            speech_clusters=[i[1] for i in speech_ids_by_speaker],\n",
    "        )\n",
    "    )\n",
    "    tf_idf_jobs[-1].to_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:49:43.886933Z",
     "start_time": "2023-05-25T14:49:43.059774Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39060\n",
      "14150\n"
     ]
    }
   ],
   "source": [
    "parl_base = Path('data/annotated/ParlaMint-GB.TEI.ana')\n",
    "parl_txt_base = Path('data/nonannotated/ParlaMint-GB.txt')\n",
    "\n",
    "id_to_party = {}\n",
    "df = pd.concat([pd.read_csv(f, delimiter='\\t') for f in  parl_txt_base.glob('2020/*-commons-meta.tsv')])\n",
    "\n",
    "job = TFIDFJob(\n",
    "    tf_idf_output_dir=Path('artefacts/tf_idf/0001-test'),\n",
    "    speech_cluster_labels=[\n",
    "        'conservative',\n",
    "        'labour',\n",
    "    ],\n",
    "    speech_clusters=[\n",
    "        set(df[df['Speaker_party'] == 'CON']['ID']),\n",
    "        set(df[df['Speaker_party'] == 'LAB']['ID']),\n",
    "    ],\n",
    ")\n",
    "tf_idf_jobs = [job]\n",
    "print(len(job.speech_clusters[0]))\n",
    "print(len(job.speech_clusters[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:32.887100Z",
     "start_time": "2023-05-25T14:36:10.718536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parl_base = Path('data/annotated/ParlaMint-GB.TEI.ana')\n",
    "job = TFIDFJob(\n",
    "    tf_idf_output_dir=Path('artefacts/tf_idf/0001-test'),\n",
    "    speech_cluster_labels=[\n",
    "        '03-04 - 0.769',\n",
    "        '01-15 - 0.758',\n",
    "        '11-25 - 0.749',\n",
    "        '02-05 - 0.000',\n",
    "        '11-20 - 0.006',\n",
    "        '09-11 - 0.015',\n",
    "    ],\n",
    "    speech_clusters=[\n",
    "        discover_speech_ids(list(parl_base.glob('2019/*03-04*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2018/*01-15*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2020/*11-25*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2016/*02-05*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2015/*11-20*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2020/*09-11*commons*')))\n",
    "    ],\n",
    ")\n",
    "tf_idf_jobs = [job]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:49:49.375644Z",
     "start_time": "2023-05-25T14:49:48.329120Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech2xml = pd.read_feather('artefacts/mappings/speech2xml.feather').set_index('speech_id')\n",
    "for job in tf_idf_jobs:\n",
    "    job.tf_idf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    job.corpus_documents = []\n",
    "    for speech_cluster in job.speech_clusters:\n",
    "        job.corpus_documents.append((speech_cluster, set(speech2xml.loc[list(speech_cluster), 'xml_path'].unique())))\n",
    "\n",
    "    # used for multiprocessing\n",
    "    job.mp_corpus_documents = []\n",
    "    for i, (speech_cluster, xml_files) in enumerate(job.corpus_documents):\n",
    "        for xml_file in xml_files:\n",
    "            job.mp_corpus_documents.append([i, speech_cluster, xml_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T15:02:18.944270Z",
     "start_time": "2023-05-25T15:01:34.339426Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████▌                                                   | 151/302 [00:43<00:43,  3.48it/s]Process ForkPoolWorker-772:\n",
      "Process ForkPoolWorker-776:\n",
      "Process ForkPoolWorker-783:\n",
      "Process ForkPoolWorker-782:\n",
      "Process ForkPoolWorker-778:\n",
      "Process ForkPoolWorker-797:\n",
      "Process ForkPoolWorker-786:\n",
      "Process ForkPoolWorker-795:\n",
      "Process ForkPoolWorker-789:\n",
      "Process ForkPoolWorker-792:\n",
      "Process ForkPoolWorker-800:\n",
      "Process ForkPoolWorker-791:\n",
      "Process ForkPoolWorker-790:\n",
      "Process ForkPoolWorker-787:\n",
      "Process ForkPoolWorker-771:\n",
      "Process ForkPoolWorker-779:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-780:\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_620267/3350495156.py\", line 6, in get_document_lemmas\n",
      "    soup = BeautifulSoup(contents, 'xml')\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_620267/3350495156.py\", line 6, in get_document_lemmas\n",
      "    soup = BeautifulSoup(contents, 'xml')\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/site-packages/bs4/__init__.py\", line 335, in __init__\n",
      "    self._feed()\n",
      "  File \"/tmp/ipykernel_620267/3350495156.py\", line 6, in get_document_lemmas\n",
      "    soup = BeautifulSoup(contents, 'xml')\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/site-packages/bs4/__init__.py\", line 478, in _feed\n",
      "    self.builder.feed(self.markup)\n",
      "  File \"/home/vidklopcic/anaconda3/envs/dhh23/lib/python3.9/site-packages/bs4/__init__.py\", line 335, in __init__\n",
      "    self._feed()\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m df_per_lemma \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mmultiprocessing\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cluster_i, result \u001b[38;5;129;01min\u001b[39;00m tqdm(pool\u001b[38;5;241m.\u001b[39mimap_unordered(get_document_lemmas, job\u001b[38;5;241m.\u001b[39mmp_corpus_documents),\n\u001b[1;32m     27\u001b[0m                                   total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(job\u001b[38;5;241m.\u001b[39mmp_corpus_documents)):\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cluster_i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tf_per_cluster_id:\n\u001b[1;32m     29\u001b[0m             tf_per_cluster_id[cluster_i] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/dhh23/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dhh23/lib/python3.9/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/envs/dhh23/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_document_lemmas(document):\n",
    "    speech_cluster_i, speech_cluster, xml_file = document\n",
    "    lemmas = {}\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "    document_speeches = {i.get('xml:id'): i for i in soup.find_all('u')}\n",
    "    for speech_id in speech_cluster:\n",
    "        sentence = document_speeches.get(speech_id)\n",
    "        if not sentence:\n",
    "            continue\n",
    "        for word in sentence.find_all('w'):\n",
    "            lemma: str = word.get('lemma')\n",
    "            if not lemma or not lemma.isalpha():\n",
    "                continue\n",
    "            lemma = lemma.lower()\n",
    "            lemmas[lemma] = lemmas.get(lemma, 0) + 1\n",
    "    return speech_cluster_i, lemmas\n",
    "\n",
    "\n",
    "df = None\n",
    "for job in tf_idf_jobs:\n",
    "    tf_per_cluster_id = {}\n",
    "    df_per_lemma = {}\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        for cluster_i, result in tqdm(pool.imap_unordered(get_document_lemmas, job.mp_corpus_documents),\n",
    "                                      total=len(job.mp_corpus_documents)):\n",
    "            if cluster_i not in tf_per_cluster_id:\n",
    "                tf_per_cluster_id[cluster_i] = {}\n",
    "            for lemma in result:\n",
    "                tf_per_cluster_id[cluster_i][lemma] = tf_per_cluster_id[cluster_i].get(lemma, 0) + result[lemma]\n",
    "                if lemma not in df_per_lemma:\n",
    "                    df_per_lemma[lemma] = set()\n",
    "                df_per_lemma[lemma].add(cluster_i)\n",
    "\n",
    "    df_per_lemma = {lemma: len(df_per_lemma[lemma]) for lemma in df_per_lemma}\n",
    "    tf_per_cluster = [tf_per_cluster_id[i] for i in range(len(job.speech_clusters))]\n",
    "    print('Calculating TF-IDF')\n",
    "    idf_per_cluster = []\n",
    "    for i, tfs in enumerate(tf_per_cluster):\n",
    "        tf_idf = {}\n",
    "        max_tf = max(tfs.values())\n",
    "        for lemma, tf in tfs.items():\n",
    "            print(tf)\n",
    "            df = df_per_lemma[lemma]\n",
    "            idf = math.log(len(job.speech_clusters) / df) if df else 0  # IDF\n",
    "            tf = 0.5 + 0.5 * tf / max_tf\n",
    "            tf_idf[lemma] = tf * idf\n",
    "        idf_per_cluster.append(tf_idf)\n",
    "    print('Saving')\n",
    "    with open(job.tf_idf_output_dir / f'tf_df_values.json', 'w') as f:\n",
    "        json.dump(idf_per_cluster, f)\n",
    "\n",
    "    if len(tf_idf_jobs) == 1:\n",
    "        data = {}\n",
    "        for label, idf in zip(job.speech_cluster_labels, idf_per_cluster):\n",
    "            data[label], data[f'{label}_scores'] = zip(*sorted(idf.items(), key=lambda x: x[1], reverse=True))\n",
    "        df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T22:48:16.084700Z",
     "start_time": "2023-05-24T21:41:52.456935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2209/2209 [49:09<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ES-GA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [03:05<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing HU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 515/515 [07:02<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing UA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1091/1091 [04:25<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_sentence_scores(args):\n",
    "    lemma_frequencies, document_count, xml_file = args\n",
    "    with open(lemma_frequencies, 'r') as f:\n",
    "        lemma_frequencies = json.load(f)\n",
    "\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "\n",
    "    scores: dict[str, dict[str, int]] = {}\n",
    "    speaker_ids = set([i.get('who') for i in soup.find_all('u') if i.get('who')])\n",
    "    for speaker_id in speaker_ids:\n",
    "        u_elements = soup.find_all('u', {'who': speaker_id})\n",
    "        sentences = [u.find_all('s') for u in u_elements]\n",
    "        sentences = [item for sublist in sentences for item in sublist]\n",
    "\n",
    "        speaker_tf = {}\n",
    "        speaker_sentences = {}\n",
    "        for sentence in sentences:\n",
    "            sentence_id = sentence.get('xml:id')\n",
    "            speaker_sentences[sentence_id] = []\n",
    "            for word in sentence.find_all('w'):\n",
    "                lemma: str = word.get('lemma')\n",
    "                if not lemma or not lemma.isalpha():\n",
    "                    continue\n",
    "                lemma = lemma.lower()\n",
    "                speaker_tf[lemma] = speaker_tf.get(lemma, 0) + 1\n",
    "                speaker_sentences[sentence_id].append(lemma)\n",
    "\n",
    "        for sentence_id, sentence in speaker_sentences.items():\n",
    "            scores[sentence_id] = []\n",
    "            for lemma in sentence:\n",
    "                tf = speaker_tf[lemma]\n",
    "                df = lemma_frequencies[lemma]\n",
    "                idf = math.log(document_count / df) if df else 0  # IDF\n",
    "                score = tf * idf  # TF-IDF\n",
    "                scores[sentence_id].append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def process_parlamint(parlamint):\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        sentences = {}\n",
    "        for result in tqdm(pool.imap_unordered(get_sentence_scores, [\n",
    "            (artefacts / f'{i[\"language\"]}_lemma_frequencies.json', len(parlamint), i['xml_path']) for i in parlamint\n",
    "        ]), total=len(parlamint)):\n",
    "            sentences.update(result)\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_scores.json', 'w') as f:\n",
    "        json.dump(sentences, f)\n",
    "\n",
    "\n",
    "for parlamint in parlamints:\n",
    "    process_parlamint(parlamint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T05:39:46.332327Z",
     "start_time": "2023-05-25T05:36:55.022903Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing GB\n",
      "max_score 686.7005062402968\n",
      "processing ES-GA\n",
      "max_score 183.85466095245155\n",
      "processing HU\n",
      "max_score 255.82151674556403\n",
      "processing UA\n",
      "max_score 672.2312987938257\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for parlamint in parlamints[1:]:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_scores.json', 'r') as f:\n",
    "        sentence_scores = json.load(f)\n",
    "    max_score = max([s for ss in sentence_scores.values() for s in ss])\n",
    "    print('max_score', max_score)\n",
    "    sentece_weights = {}\n",
    "    for sentence_id in sentence_scores:\n",
    "        sentece_weights[sentence_id] = np.mean(sentence_scores[sentence_id]) / max_score\n",
    "\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'w') as f:\n",
    "        json.dump(sentece_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T21:40:57.683428Z",
     "start_time": "2023-05-24T21:37:34.219910Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing SI\n",
      "243 ventilatorjev\n",
      "453 ventilatorjev\n",
      "Merilniki slanosti\n",
      "404 ventilatorji 13. marca\n",
      "Prehajamo na podprogram Štipendije\n",
      "Podjetje Lastinski je prišlo po merilnike slanosti k Darsu\n",
      "Multinacionalka je prodala žilno opornico recimo Mark Medicalu S. p.\n",
      "Oklepnike\n",
      "Ceca\n",
      "Prehajamo na podprogram Dolgotrajna oskrba\n",
      "Prehajamo na podprogram Univerzitetne knjižnice\n",
      "Prehajamo na podprogram Kakovost zraka\n",
      "Lastniki so bili Mark Medical S. p.\n",
      "Prva pogodba med Darsom in podjetjem Lastinski\n",
      "To se pravi podjetje Lastinski je vrnilo merilnike slanosti\n",
      "Palestina\n",
      "Palestina\n",
      "Burka\n",
      "Nazaj k ventilatorjem\n",
      "Mark Medical Slovenija je v lastniški družbi KB 1909\n",
      "Tako pride do posla podjetje Lastinski\n",
      "Število ventilatorjev\n",
      "Število ventilatorjev\n",
      "Prehajamo na podprogram Podpora raziskovalni infrastrukturi\n",
      "Prehajamo na podprogram Podpora raziskovalni infrastrukturi\n",
      "Pomožnega motorja\n",
      "Prehajamo na podprogram Kreiranje delovnih mest\n",
      "Prehajamo na podprogram Kreiranje delovnih mest\n",
      "Poglejte kaj je pa firma Lastinski odgovorila Darsu\n",
      "Prehajamo še na podprogram Športna infrastruktura\n",
      "Prehajamo še na podprogram Športna infrastruktura\n",
      "Prehajamo še na podprogram Umetnostni programi\n",
      "Prehajamo na podprogram Izvajanje osnovnošolskih programov\n",
      "Prehajamo na podprogram Izvajanje osnovnošolskih programov\n",
      "Prehajamo še na podprogram Dolgotrajna oskrba\n",
      "Prehajamo na podprogram Dejavnost višjega šolstva\n",
      "Prehajamo na podprogram Dejavnost visokega šolstva\n",
      "Prehajamo na podprogram Dejavnost visokega šolstva\n",
      "Prehajamo na podprogram Programi socialnega varstva\n",
      "Prehajamo na podprogram Upravljanja z vodami\n",
      "Prehajamo na podprogram Upravljanje z vodami\n",
      "Manjkalo je 130 žilnih opornic\n",
      "8. oktobra 2013 lani oktobra torej prejme Dars ponudbo podjetja Lastinski d. o. o. za umerjanje merilnikov slanosti\n",
      "Prehajamo na podprogram Založništvo\n",
      "Odločamo o amandmaju poslanskih skupin LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin LMŠ in SAB k temu podprogramu\n",
      "Še glede podelitve koncesije Termam Čatež\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "--------\n",
      "sejo\n",
      "Državnega zbora\n",
      "Da\n",
      "Da\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Državni zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani Zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "def print_ids(ids):\n",
    "    def get_path(id):\n",
    "        year = id.split('_')[1].split('-')[0]\n",
    "        return f'data/annotated/ParlaMint-SI.TEI.ana/{year}/{id.split(\".seg\")[0]}.xml'\n",
    "\n",
    "    soups = {}\n",
    "    for sid in ids:\n",
    "        xml_path = get_path(sid)\n",
    "        if xml_path not in soups:\n",
    "            with open(xml_path, 'r') as f:\n",
    "                contents = f.read()\n",
    "            soups[xml_path] = BeautifulSoup(contents, 'xml')\n",
    "        soup = soups[xml_path]\n",
    "        sentence = soup.find('s', {'xml:id': sid})\n",
    "        print(' '.join([w.get_text() for w in sentence.find_all('w')]))\n",
    "\n",
    "\n",
    "for parlamint in parlamints[:1]:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'r') as f:\n",
    "        sentece_weights = json.load(f)\n",
    "        top_50 = heapq.nlargest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(top_50)\n",
    "        print('--------')\n",
    "        bottom_50 = heapq.nsmallest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(bottom_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for parlamint in parlamints:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'r') as f:\n",
    "        sentece_weights = json.load(f)\n",
    "        top_50 = heapq.nlargest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(top_50)\n",
    "        print('--------')\n",
    "        bottom_50 = heapq.nsmallest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(bottom_50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
