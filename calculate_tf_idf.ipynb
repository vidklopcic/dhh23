{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:35:54.859083Z",
     "start_time": "2023-05-25T14:35:54.638039Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['language', 'date', 'xml_path', 'embeddings_path'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('artefacts/parlamint_with_embeddings.json', 'r') as f:\n",
    "    parlamint = json.load(f)\n",
    "parlamint_es_ga = parlamint['ES-GA']\n",
    "parlaming_gb = parlamint['GB']\n",
    "parlamint_hu = parlamint['HU']\n",
    "parlamint_ua = parlamint['UA']\n",
    "parlamint_si = parlamint['SI']\n",
    "parlamints = [\n",
    "    parlamint_si,\n",
    "    parlamint_es_ga,\n",
    "    parlaming_gb,\n",
    "    parlamint_hu,\n",
    "    parlamint_ua,\n",
    "]\n",
    "artefacts = Path('artefacts/tf_idf')\n",
    "parlamint_si[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create mappings for speech ids to xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:04.062413Z",
     "start_time": "2023-05-25T14:35:58.908905Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                | 0/1572 [00:02<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def worker(document):\n",
    "    rows = []\n",
    "    with open(document['xml_path'], 'r', encoding='utf8') as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "    for speech in soup.find_all('u'):\n",
    "        speech_id = speech.get('xml:id')\n",
    "        rows.append([speech_id, document['xml_path']])\n",
    "    return rows\n",
    "\n",
    "\n",
    "columns = ['speech_id', 'xml_path']\n",
    "all_rows = []\n",
    "\n",
    "# Create a multiprocessing pool\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    for parlamint in parlamints:\n",
    "        # Use pool.map or pool.imap to run the worker function in parallel on each document\n",
    "        # tqdm can be used in conjunction with pool.imap for a progress bar\n",
    "        for result in tqdm(pool.imap(worker, parlamint), total=len(parlamint)):\n",
    "            all_rows.extend(result)\n",
    "\n",
    "df = pd.DataFrame(all_rows, columns=columns)\n",
    "df.to_feather('artefacts/mappings/speech2xml.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Parameters\n",
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:05.562974Z",
     "start_time": "2023-05-25T14:36:05.562688Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TFIDFJob:\n",
    "    tf_idf_output_dir: Path\n",
    "    speech_cluster_labels: list[str]\n",
    "    speech_clusters: list[set[str]]\n",
    "    corpus_documents: Optional[Any] = None\n",
    "    mp_corpus_documents: Optional[Any] = None\n",
    "\n",
    "    @classmethod\n",
    "    def pickle_path(cls, tf_idf_output_dir: Path):\n",
    "        return tf_idf_output_dir / 'job.pickle'\n",
    "\n",
    "    def to_pickle(self):\n",
    "        with open(self.pickle_path(self.tf_idf_output_dir), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'tf_idf_output_dir': self.tf_idf_output_dir,\n",
    "                'speech_cluster_labels': self.speech_cluster_labels,\n",
    "                'speech_clusters': self.speech_clusters,\n",
    "            }, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pickle(cls, tf_idf_output_dir: Path):\n",
    "        pickle_path = cls.pickle_path(tf_idf_output_dir)\n",
    "        if not pickle_path.exists():\n",
    "            return None\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "def discover_speech_ids(xml_documents: list[str]):\n",
    "    speech_ids = set()\n",
    "    for xml_file in xml_documents:\n",
    "        with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "            contents = f.read()\n",
    "        soup = BeautifulSoup(contents, 'xml')\n",
    "        for speech in soup.find_all('u'):\n",
    "            speech_id = speech.get('xml:id')\n",
    "            speech_ids.add(speech_id)\n",
    "    return speech_ids\n",
    "\n",
    "\n",
    "def get_speech_ids_by_speaker_worker(xml_file):\n",
    "    speech_ids_by_speaker = defaultdict(set)\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "    for speech in soup.find_all('u'):\n",
    "        speech_id = speech.get('xml:id')\n",
    "        speaker = speech.get('who')\n",
    "        speech_ids_by_speaker[speaker].add(speech_id)\n",
    "    return dict(speech_ids_by_speaker)\n",
    "\n",
    "\n",
    "def get_speech_ids_by_speaker(xml_documents: list[str]):\n",
    "    speech_ids_by_speaker = defaultdict(set)\n",
    "\n",
    "    # Create a multiprocessing pool\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        # Use pool.imap to run the worker function in parallel on each xml_file\n",
    "        # tqdm can be used in conjunction with pool.imap for a progress bar\n",
    "        for result in tqdm(pool.imap(get_speech_ids_by_speaker_worker, xml_documents), total=len(xml_documents)):\n",
    "            # Merge the result into the final dictionary\n",
    "            for speaker, speech_ids in result.items():\n",
    "                speech_ids_by_speaker[speaker].update(speech_ids)\n",
    "\n",
    "    return dict(speech_ids_by_speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### TF-IDF per speaker for language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_jobs = []\n",
    "for parlamint in parlamints:\n",
    "    print('discovering speech ids by speaker for', parlamint[0]['language'])\n",
    "    speech_ids_by_speaker = get_speech_ids_by_speaker([d['xml_path'] for d in parlamint])\n",
    "    speech_ids_by_speaker = list(speech_ids_by_speaker.items())\n",
    "    tf_idf_output_dir = artefacts / 'by_speaker' / parlamint[0][\"language\"]\n",
    "    job = TFIDFJob.from_pickle(tf_idf_output_dir)\n",
    "    if job:\n",
    "        tf_idf_jobs.append(job)\n",
    "        continue\n",
    "    tf_idf_jobs.append(\n",
    "        TFIDFJob(\n",
    "            tf_idf_output_dir=tf_idf_output_dir,\n",
    "            speech_cluster_labels=[i[0] for i in speech_ids_by_speaker],\n",
    "            speech_clusters=[i[1] for i in speech_ids_by_speaker],\n",
    "        )\n",
    "    )\n",
    "    tf_idf_jobs[-1].to_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:32.887100Z",
     "start_time": "2023-05-25T14:36:10.718536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parl_base = Path('data/annotated/ParlaMint-GB.TEI.ana')\n",
    "job = TFIDFJob(\n",
    "    tf_idf_output_dir=Path('artefacts/tf_idf/0001-test'),\n",
    "    speech_cluster_labels=[\n",
    "        '03-04 - 0.769',\n",
    "        '01-15 - 0.758',\n",
    "        '11-25 - 0.749',\n",
    "        '02-05 - 0.000',\n",
    "        '11-20 - 0.006',\n",
    "        '09-11 - 0.015',\n",
    "    ],\n",
    "    speech_clusters=[\n",
    "        discover_speech_ids(list(parl_base.glob('2019/*03-04*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2018/*01-15*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2020/*11-25*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2016/*02-05*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2015/*11-20*commons*'))),\n",
    "        discover_speech_ids(list(parl_base.glob('2020/*09-11*commons*')))\n",
    "    ],\n",
    ")\n",
    "tf_idf_jobs = [job]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:35.035287Z",
     "start_time": "2023-05-25T14:36:33.867669Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speech2xml = pd.read_feather('artefacts/mappings/speech2xml.feather').set_index('speech_id')\n",
    "for job in tf_idf_jobs:\n",
    "    job.tf_idf_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    job.corpus_documents = []\n",
    "    for speech_cluster in job.speech_clusters:\n",
    "        job.corpus_documents.append((speech_cluster, set(speech2xml.loc[list(speech_cluster), 'xml_path'].unique())))\n",
    "\n",
    "    # used for multiprocessing\n",
    "    job.mp_corpus_documents = []\n",
    "    for i, (speech_cluster, xml_files) in enumerate(job.corpus_documents):\n",
    "        for xml_file in xml_files:\n",
    "            job.mp_corpus_documents.append([i, speech_cluster, xml_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:36:44.023368Z",
     "start_time": "2023-05-25T14:36:37.457376Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:05<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TF-IDF\n",
      "Saving\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>03-04 - 0.769</th>\n",
       "      <th>03-04 - 0.769_scores</th>\n",
       "      <th>01-15 - 0.758</th>\n",
       "      <th>01-15 - 0.758_scores</th>\n",
       "      <th>11-25 - 0.749</th>\n",
       "      <th>11-25 - 0.749_scores</th>\n",
       "      <th>02-05 - 0.000</th>\n",
       "      <th>02-05 - 0.000_scores</th>\n",
       "      <th>11-20 - 0.006</th>\n",
       "      <th>11-20 - 0.006_scores</th>\n",
       "      <th>09-11 - 0.015</th>\n",
       "      <th>09-11 - 0.015_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eurotunnel</td>\n",
       "      <td>0.900714</td>\n",
       "      <td>carillion</td>\n",
       "      <td>0.924636</td>\n",
       "      <td>cptpp</td>\n",
       "      <td>0.901609</td>\n",
       "      <td>riot</td>\n",
       "      <td>0.937423</td>\n",
       "      <td>cpr</td>\n",
       "      <td>0.928249</td>\n",
       "      <td>internship</td>\n",
       "      <td>0.903606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reoffending</td>\n",
       "      <td>0.899971</td>\n",
       "      <td>satellite</td>\n",
       "      <td>0.921464</td>\n",
       "      <td>icgs</td>\n",
       "      <td>0.900081</td>\n",
       "      <td>ccrc</td>\n",
       "      <td>0.922973</td>\n",
       "      <td>bma</td>\n",
       "      <td>0.911113</td>\n",
       "      <td>mcmorrin</td>\n",
       "      <td>0.903270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uprate</td>\n",
       "      <td>0.899599</td>\n",
       "      <td>spaceport</td>\n",
       "      <td>0.911526</td>\n",
       "      <td>uplift</td>\n",
       "      <td>0.899508</td>\n",
       "      <td>harry</td>\n",
       "      <td>0.902804</td>\n",
       "      <td>headteacher</td>\n",
       "      <td>0.908256</td>\n",
       "      <td>mini</td>\n",
       "      <td>0.899911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reoffend</td>\n",
       "      <td>0.897925</td>\n",
       "      <td>xanax</td>\n",
       "      <td>0.908566</td>\n",
       "      <td>harassment</td>\n",
       "      <td>0.899508</td>\n",
       "      <td>tottenham</td>\n",
       "      <td>0.902804</td>\n",
       "      <td>defibrillator</td>\n",
       "      <td>0.907939</td>\n",
       "      <td>demutualisation</td>\n",
       "      <td>0.899239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformative</td>\n",
       "      <td>0.897925</td>\n",
       "      <td>zoe</td>\n",
       "      <td>0.906452</td>\n",
       "      <td>vaccine</td>\n",
       "      <td>0.898935</td>\n",
       "      <td>miscarriage</td>\n",
       "      <td>0.902804</td>\n",
       "      <td>cardiac</td>\n",
       "      <td>0.906035</td>\n",
       "      <td>issuance</td>\n",
       "      <td>0.899239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>walk</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wholly</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>occur</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maintenance</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>primary</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4385 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       03-04 - 0.769  03-04 - 0.769_scores 01-15 - 0.758  \\\n",
       "0         eurotunnel              0.900714     carillion   \n",
       "1        reoffending              0.899971     satellite   \n",
       "2             uprate              0.899599     spaceport   \n",
       "3           reoffend              0.897925         xanax   \n",
       "4     transformative              0.897925           zoe   \n",
       "...              ...                   ...           ...   \n",
       "4380             NaN                   NaN           NaN   \n",
       "4381             NaN                   NaN           NaN   \n",
       "4382             NaN                   NaN           NaN   \n",
       "4383             NaN                   NaN           NaN   \n",
       "4384             NaN                   NaN           NaN   \n",
       "\n",
       "      01-15 - 0.758_scores 11-25 - 0.749  11-25 - 0.749_scores 02-05 - 0.000  \\\n",
       "0                 0.924636         cptpp              0.901609          riot   \n",
       "1                 0.921464          icgs              0.900081          ccrc   \n",
       "2                 0.911526        uplift              0.899508         harry   \n",
       "3                 0.908566    harassment              0.899508     tottenham   \n",
       "4                 0.906452       vaccine              0.898935   miscarriage   \n",
       "...                    ...           ...                   ...           ...   \n",
       "4380                   NaN          walk              0.000000           NaN   \n",
       "4381                   NaN        wholly              0.000000           NaN   \n",
       "4382                   NaN         occur              0.000000           NaN   \n",
       "4383                   NaN   maintenance              0.000000           NaN   \n",
       "4384                   NaN       primary              0.000000           NaN   \n",
       "\n",
       "      02-05 - 0.000_scores  11-20 - 0.006  11-20 - 0.006_scores  \\\n",
       "0                 0.937423            cpr              0.928249   \n",
       "1                 0.922973            bma              0.911113   \n",
       "2                 0.902804    headteacher              0.908256   \n",
       "3                 0.902804  defibrillator              0.907939   \n",
       "4                 0.902804        cardiac              0.906035   \n",
       "...                    ...            ...                   ...   \n",
       "4380                   NaN            NaN                   NaN   \n",
       "4381                   NaN            NaN                   NaN   \n",
       "4382                   NaN            NaN                   NaN   \n",
       "4383                   NaN            NaN                   NaN   \n",
       "4384                   NaN            NaN                   NaN   \n",
       "\n",
       "        09-11 - 0.015  09-11 - 0.015_scores  \n",
       "0          internship              0.903606  \n",
       "1            mcmorrin              0.903270  \n",
       "2                mini              0.899911  \n",
       "3     demutualisation              0.899239  \n",
       "4            issuance              0.899239  \n",
       "...               ...                   ...  \n",
       "4380              NaN                   NaN  \n",
       "4381              NaN                   NaN  \n",
       "4382              NaN                   NaN  \n",
       "4383              NaN                   NaN  \n",
       "4384              NaN                   NaN  \n",
       "\n",
       "[4385 rows x 12 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_document_lemmas(document):\n",
    "    speech_cluster_i, speech_cluster, xml_file = document\n",
    "    lemmas = {}\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "    document_speeches = {i.get('xml:id'): i for i in soup.find_all('u')}\n",
    "    for speech_id in speech_cluster:\n",
    "        sentence = document_speeches.get(speech_id)\n",
    "        if not sentence:\n",
    "            continue\n",
    "        for word in sentence.find_all('w'):\n",
    "            lemma: str = word.get('lemma')\n",
    "            if not lemma or not lemma.isalpha():\n",
    "                continue\n",
    "            lemma = lemma.lower()\n",
    "            lemmas[lemma] = lemmas.get(lemma, 0) + 1\n",
    "    return speech_cluster_i, lemmas\n",
    "\n",
    "\n",
    "df = None\n",
    "for job in tf_idf_jobs:\n",
    "    tf_per_cluster_id = {}\n",
    "    df_per_lemma = {}\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        for cluster_i, result in tqdm(pool.imap_unordered(get_document_lemmas, job.mp_corpus_documents),\n",
    "                                      total=len(job.mp_corpus_documents)):\n",
    "            if cluster_i not in tf_per_cluster_id:\n",
    "                tf_per_cluster_id[cluster_i] = {}\n",
    "            for lemma in result:\n",
    "                tf_per_cluster_id[cluster_i][lemma] = tf_per_cluster_id[cluster_i].get(lemma, 0) + result[lemma]\n",
    "                if lemma not in df_per_lemma:\n",
    "                    df_per_lemma[lemma] = set()\n",
    "                df_per_lemma[lemma].add(cluster_i)\n",
    "\n",
    "    df_per_lemma = {lemma: len(df_per_lemma[lemma]) for lemma in df_per_lemma}\n",
    "    tf_per_cluster = [tf_per_cluster_id[i] for i in range(len(job.speech_clusters))]\n",
    "    print('Calculating TF-IDF')\n",
    "    idf_per_cluster = []\n",
    "    for i, tfs in enumerate(tf_per_cluster):\n",
    "        tf_idf = {}\n",
    "        max_tf = max(tfs.values())\n",
    "        for lemma, tf in tfs.items():\n",
    "            df = df_per_lemma[lemma]\n",
    "            idf = math.log(len(job.speech_clusters) / df) if df else 0  # IDF\n",
    "            tf = 0.5 + 0.5 * tf / max_tf\n",
    "            tf_idf[lemma] = tf * idf\n",
    "        idf_per_cluster.append(tf_idf)\n",
    "    print('Saving')\n",
    "    with open(job.tf_idf_output_dir / f'tf_df_values.json', 'w') as f:\n",
    "        json.dump(idf_per_cluster, f)\n",
    "\n",
    "    if len(tf_idf_jobs) == 1:\n",
    "        data = {}\n",
    "        for label, idf in zip(job.speech_cluster_labels, idf_per_cluster):\n",
    "            data[label], data[f'{label}_scores'] = zip(*sorted(idf.items(), key=lambda x: x[1], reverse=True))\n",
    "        df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data.items()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T22:48:16.084700Z",
     "start_time": "2023-05-24T21:41:52.456935Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2209/2209 [49:09<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ES-GA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 302/302 [03:05<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing HU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 515/515 [07:02<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing UA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1091/1091 [04:25<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_sentence_scores(args):\n",
    "    lemma_frequencies, document_count, xml_file = args\n",
    "    with open(lemma_frequencies, 'r') as f:\n",
    "        lemma_frequencies = json.load(f)\n",
    "\n",
    "    with open(xml_file, \"r\", encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'xml')\n",
    "\n",
    "    scores: dict[str, dict[str, int]] = {}\n",
    "    speaker_ids = set([i.get('who') for i in soup.find_all('u') if i.get('who')])\n",
    "    for speaker_id in speaker_ids:\n",
    "        u_elements = soup.find_all('u', {'who': speaker_id})\n",
    "        sentences = [u.find_all('s') for u in u_elements]\n",
    "        sentences = [item for sublist in sentences for item in sublist]\n",
    "\n",
    "        speaker_tf = {}\n",
    "        speaker_sentences = {}\n",
    "        for sentence in sentences:\n",
    "            sentence_id = sentence.get('xml:id')\n",
    "            speaker_sentences[sentence_id] = []\n",
    "            for word in sentence.find_all('w'):\n",
    "                lemma: str = word.get('lemma')\n",
    "                if not lemma or not lemma.isalpha():\n",
    "                    continue\n",
    "                lemma = lemma.lower()\n",
    "                speaker_tf[lemma] = speaker_tf.get(lemma, 0) + 1\n",
    "                speaker_sentences[sentence_id].append(lemma)\n",
    "\n",
    "        for sentence_id, sentence in speaker_sentences.items():\n",
    "            scores[sentence_id] = []\n",
    "            for lemma in sentence:\n",
    "                tf = speaker_tf[lemma]\n",
    "                df = lemma_frequencies[lemma]\n",
    "                idf = math.log(document_count / df) if df else 0  # IDF\n",
    "                score = tf * idf  # TF-IDF\n",
    "                scores[sentence_id].append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def process_parlamint(parlamint):\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        sentences = {}\n",
    "        for result in tqdm(pool.imap_unordered(get_sentence_scores, [\n",
    "            (artefacts / f'{i[\"language\"]}_lemma_frequencies.json', len(parlamint), i['xml_path']) for i in parlamint\n",
    "        ]), total=len(parlamint)):\n",
    "            sentences.update(result)\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_scores.json', 'w') as f:\n",
    "        json.dump(sentences, f)\n",
    "\n",
    "\n",
    "for parlamint in parlamints:\n",
    "    process_parlamint(parlamint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T05:39:46.332327Z",
     "start_time": "2023-05-25T05:36:55.022903Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing GB\n",
      "max_score 686.7005062402968\n",
      "processing ES-GA\n",
      "max_score 183.85466095245155\n",
      "processing HU\n",
      "max_score 255.82151674556403\n",
      "processing UA\n",
      "max_score 672.2312987938257\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for parlamint in parlamints[1:]:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_scores.json', 'r') as f:\n",
    "        sentence_scores = json.load(f)\n",
    "    max_score = max([s for ss in sentence_scores.values() for s in ss])\n",
    "    print('max_score', max_score)\n",
    "    sentece_weights = {}\n",
    "    for sentence_id in sentence_scores:\n",
    "        sentece_weights[sentence_id] = np.mean(sentence_scores[sentence_id]) / max_score\n",
    "\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'w') as f:\n",
    "        json.dump(sentece_weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T21:40:57.683428Z",
     "start_time": "2023-05-24T21:37:34.219910Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing SI\n",
      "243 ventilatorjev\n",
      "453 ventilatorjev\n",
      "Merilniki slanosti\n",
      "404 ventilatorji 13. marca\n",
      "Prehajamo na podprogram Štipendije\n",
      "Podjetje Lastinski je prišlo po merilnike slanosti k Darsu\n",
      "Multinacionalka je prodala žilno opornico recimo Mark Medicalu S. p.\n",
      "Oklepnike\n",
      "Ceca\n",
      "Prehajamo na podprogram Dolgotrajna oskrba\n",
      "Prehajamo na podprogram Univerzitetne knjižnice\n",
      "Prehajamo na podprogram Kakovost zraka\n",
      "Lastniki so bili Mark Medical S. p.\n",
      "Prva pogodba med Darsom in podjetjem Lastinski\n",
      "To se pravi podjetje Lastinski je vrnilo merilnike slanosti\n",
      "Palestina\n",
      "Palestina\n",
      "Burka\n",
      "Nazaj k ventilatorjem\n",
      "Mark Medical Slovenija je v lastniški družbi KB 1909\n",
      "Tako pride do posla podjetje Lastinski\n",
      "Število ventilatorjev\n",
      "Število ventilatorjev\n",
      "Prehajamo na podprogram Podpora raziskovalni infrastrukturi\n",
      "Prehajamo na podprogram Podpora raziskovalni infrastrukturi\n",
      "Pomožnega motorja\n",
      "Prehajamo na podprogram Kreiranje delovnih mest\n",
      "Prehajamo na podprogram Kreiranje delovnih mest\n",
      "Poglejte kaj je pa firma Lastinski odgovorila Darsu\n",
      "Prehajamo še na podprogram Športna infrastruktura\n",
      "Prehajamo še na podprogram Športna infrastruktura\n",
      "Prehajamo še na podprogram Umetnostni programi\n",
      "Prehajamo na podprogram Izvajanje osnovnošolskih programov\n",
      "Prehajamo na podprogram Izvajanje osnovnošolskih programov\n",
      "Prehajamo še na podprogram Dolgotrajna oskrba\n",
      "Prehajamo na podprogram Dejavnost višjega šolstva\n",
      "Prehajamo na podprogram Dejavnost visokega šolstva\n",
      "Prehajamo na podprogram Dejavnost visokega šolstva\n",
      "Prehajamo na podprogram Programi socialnega varstva\n",
      "Prehajamo na podprogram Upravljanja z vodami\n",
      "Prehajamo na podprogram Upravljanje z vodami\n",
      "Manjkalo je 130 žilnih opornic\n",
      "8. oktobra 2013 lani oktobra torej prejme Dars ponudbo podjetja Lastinski d. o. o. za umerjanje merilnikov slanosti\n",
      "Prehajamo na podprogram Založništvo\n",
      "Odločamo o amandmaju poslanskih skupin LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin LMŠ in SAB k temu podprogramu\n",
      "Še glede podelitve koncesije Termam Čatež\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "Odločamo o amandmaju poslanskih skupin SD LMŠ in SAB k temu podprogramu\n",
      "--------\n",
      "sejo\n",
      "Državnega zbora\n",
      "Da\n",
      "Da\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Spoštovani državni zbor\n",
      "Državni zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani Zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n",
      "Spoštovani zbor\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "def print_ids(ids):\n",
    "    def get_path(id):\n",
    "        year = id.split('_')[1].split('-')[0]\n",
    "        return f'data/annotated/ParlaMint-SI.TEI.ana/{year}/{id.split(\".seg\")[0]}.xml'\n",
    "\n",
    "    soups = {}\n",
    "    for sid in ids:\n",
    "        xml_path = get_path(sid)\n",
    "        if xml_path not in soups:\n",
    "            with open(xml_path, 'r') as f:\n",
    "                contents = f.read()\n",
    "            soups[xml_path] = BeautifulSoup(contents, 'xml')\n",
    "        soup = soups[xml_path]\n",
    "        sentence = soup.find('s', {'xml:id': sid})\n",
    "        print(' '.join([w.get_text() for w in sentence.find_all('w')]))\n",
    "\n",
    "\n",
    "for parlamint in parlamints[:1]:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'r') as f:\n",
    "        sentece_weights = json.load(f)\n",
    "        top_50 = heapq.nlargest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(top_50)\n",
    "        print('--------')\n",
    "        bottom_50 = heapq.nsmallest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(bottom_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for parlamint in parlamints:\n",
    "    print('processing', parlamint[0]['language'])\n",
    "    with open(artefacts / f'{parlamint[0][\"language\"]}_sentence_weights.json', 'r') as f:\n",
    "        sentece_weights = json.load(f)\n",
    "        top_50 = heapq.nlargest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(top_50)\n",
    "        print('--------')\n",
    "        bottom_50 = heapq.nsmallest(50, sentece_weights, key=lambda x: sentece_weights[x])\n",
    "        print_ids(bottom_50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
