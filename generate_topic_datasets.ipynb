{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:19:22.627887Z",
     "start_time": "2023-05-30T07:19:22.135394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import stopwordsiso as stopwords\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Languages:\n",
    "    si = 'SI'\n",
    "    gb = 'GB'\n",
    "    hu = 'HU'\n",
    "    ua = 'UA'\n",
    "    all = [si, gb, hu, ua]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:20:00.381404Z",
     "start_time": "2023-05-30T07:19:25.315674Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading SI\n",
      "loading GB\n",
      "loading HU\n",
      "loading UA\n"
     ]
    }
   ],
   "source": [
    "sentences = {}\n",
    "for language in Languages.all:\n",
    "    print('loading', language)\n",
    "    sentences[language] = pd.read_feather(f'artefacts/pandas_frames/{language}_parlamint_sentences.feather')\n",
    "metadata = pd.read_feather('artefacts/pandas_frames/parlamint_metadata.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T07:20:20.921583Z",
     "start_time": "2023-05-30T07:20:02.552818Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:18<00:00,  4.56s/it]\n"
     ]
    }
   ],
   "source": [
    "artefacts_pd_frames = Path('artefacts/pandas_frames')\n",
    "speeches = {}\n",
    "for language in tqdm(sentences):\n",
    "    with open(artefacts_pd_frames / f'speech2lemmas_{language}.pkl', 'rb') as f:\n",
    "        speeches[language] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T08:12:21.581164Z",
     "start_time": "2023-05-30T08:11:27.744193Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded SI len = 311354\n",
      "loaded UA len = 195685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_649621/2632164922.py:11: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lang_df = pd.read_csv(artefacts_base / csv_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded HU len = 104521\n",
      "loaded GB len = 472782\n"
     ]
    }
   ],
   "source": [
    "artefacts_base = Path('artefacts/bojan')\n",
    "files = {\n",
    "    Languages.si: 'ParlaMint-SI_with_sentiment.csv',\n",
    "    Languages.ua: 'ParlaMint-UA_with_sentiment.csv',\n",
    "    Languages.hu: 'ParlaMint-HU_with_sentiment.csv',\n",
    "    Languages.gb: 'ParlaMint-GB-commons_with_sentiment.csv',\n",
    "}\n",
    "filtered_speeches = {}\n",
    "filtered_speech_ids = set()\n",
    "for language, csv_file in files.items():\n",
    "    lang_df = pd.read_csv(artefacts_base / csv_file)\n",
    "    print('loaded', language, 'len =', len(lang_df))\n",
    "    lang_df = lang_df[lang_df['Speaker_role'] == 'Regular']\n",
    "    lang_df = lang_df[lang_df['Speaker_MP'] == 'MP']\n",
    "    lang_df['speech_length'] = lang_df['speech'].apply(lambda x: len(x) if type(x) == str else 0)\n",
    "    lang_df = lang_df[lang_df['speech_length'] > 200]\n",
    "    filtered_speech_ids.update(lang_df.ID)\n",
    "    filtered_speeches[language] = lang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:51:29.760216Z",
     "start_time": "2023-05-30T09:51:29.705578Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_speech_ids(speech2lemmas: dict[str, set[str]], keywords: set[str]):\n",
    "    for speech_id, lemmas in speech2lemmas.items():\n",
    "        speech_id = speech_id.replace('.ana', '')\n",
    "        if speech_id not in filtered_speech_ids:\n",
    "            continue\n",
    "        lemmas = f' {lemmas.lower()} '\n",
    "        for keyword in keywords:\n",
    "            if f' {keyword} ' in lemmas:\n",
    "                yield speech_id\n",
    "                break\n",
    "\n",
    "\n",
    "def get_corpuses(topic_by_language: dict[str, set[str]]):\n",
    "    corpuses = {}\n",
    "    for language, speech2lemmas in speeches.items():\n",
    "        speech_ids = list(get_speech_ids(speech2lemmas, topic_by_language[language]))\n",
    "        corpuses[language] = speech_ids\n",
    "    return corpuses\n",
    "\n",
    "\n",
    "def print_corpuses(corpuses):\n",
    "    for lang, corpus in corpuses.items():\n",
    "        print(f'{lang}:', len(corpus))\n",
    "        print(corpus[:10])\n",
    "        print('-' * 40)\n",
    "\n",
    "\n",
    "def save_corpuses(topic, corpuses):\n",
    "    artefacts_dir = Path('artefacts/by_topic')\n",
    "    artefacts_dir.mkdir(exist_ok=True)\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        'common': set([t for l in corpuses.values() for t in l]),\n",
    "        **corpuses\n",
    "    }, orient='index').transpose()\n",
    "    df.to_feather(artefacts_dir / f'{topic}.feather')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:28:14.272416Z",
     "start_time": "2023-05-31T07:28:13.366596Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eu', 'war', 'healthcare', 'gender'])\n",
      "dict_keys(['UK', 'Hungary', 'Slovenian', 'Ukrainian'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://docs.google.com/spreadsheets/d/1v7Kv1kVPWhBwGMDI6g2-pfanTfKuys8mmp1L0sMsMsM/export?format=csv&gid=0'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "topics = {}\n",
    "for column in df.columns:\n",
    "    if '_tr' in column or not column.strip():\n",
    "        continue\n",
    "    theme, lang = column.split(' (')\n",
    "    theme = theme.split(': ')[1].lower()\n",
    "    lang = lang.rstrip(')')\n",
    "    if theme not in topics:\n",
    "        topics[theme] = {}\n",
    "    topics[theme][lang] = [i.lower() for i in df[column].tolist() if type(i) is str and i]\n",
    "\n",
    "print(topics.keys())\n",
    "print(topics['healthcare'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:29:47.498740Z",
     "start_time": "2023-05-31T07:28:16.363371Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:31<00:00, 22.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>SI_count</th>\n",
       "      <th>SI_percentage</th>\n",
       "      <th>GB_count</th>\n",
       "      <th>GB_percentage</th>\n",
       "      <th>HU_count</th>\n",
       "      <th>HU_percentage</th>\n",
       "      <th>UA_count</th>\n",
       "      <th>UA_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eu</td>\n",
       "      <td>13943</td>\n",
       "      <td>11.287137</td>\n",
       "      <td>38938</td>\n",
       "      <td>10.258667</td>\n",
       "      <td>5933</td>\n",
       "      <td>13.969861</td>\n",
       "      <td>5863</td>\n",
       "      <td>8.361977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>war</td>\n",
       "      <td>8686</td>\n",
       "      <td>7.031490</td>\n",
       "      <td>15039</td>\n",
       "      <td>3.962199</td>\n",
       "      <td>2141</td>\n",
       "      <td>5.041206</td>\n",
       "      <td>7101</td>\n",
       "      <td>10.127647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>healthcare</td>\n",
       "      <td>13802</td>\n",
       "      <td>11.172994</td>\n",
       "      <td>45022</td>\n",
       "      <td>11.861567</td>\n",
       "      <td>4583</td>\n",
       "      <td>10.791147</td>\n",
       "      <td>4546</td>\n",
       "      <td>6.483634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gender</td>\n",
       "      <td>943</td>\n",
       "      <td>0.763377</td>\n",
       "      <td>3740</td>\n",
       "      <td>0.985346</td>\n",
       "      <td>121</td>\n",
       "      <td>0.284907</td>\n",
       "      <td>122</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic  SI_count  SI_percentage  GB_count  GB_percentage  HU_count  \\\n",
       "0          eu     13943      11.287137     38938      10.258667      5933   \n",
       "1         war      8686       7.031490     15039       3.962199      2141   \n",
       "2  healthcare     13802      11.172994     45022      11.861567      4583   \n",
       "3      gender       943       0.763377      3740       0.985346       121   \n",
       "\n",
       "   HU_percentage  UA_count  UA_percentage  \n",
       "0      13.969861      5863       8.361977  \n",
       "1       5.041206      7101      10.127647  \n",
       "2      10.791147      4546       6.483634  \n",
       "3       0.284907       122       0.174000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data = []\n",
    "languages = {\n",
    "    'UK': Languages.gb,\n",
    "    'Hungary': Languages.hu,\n",
    "    'Slovenian': Languages.si,\n",
    "    'Ukrainian': Languages.ua,\n",
    "}\n",
    "topic_dfs = {}\n",
    "for topic in tqdm(topics):\n",
    "    row_dict = {'topic': topic}\n",
    "    keywords = {}\n",
    "    for language in topics[topic]:\n",
    "        keywords[languages[language]] = set(topics[topic][language])\n",
    "    topic_df = save_corpuses(topic, get_corpuses(keywords))\n",
    "    topic_dfs[topic] = topic_df\n",
    "    topic_df_count = topic_df.count().to_dict()\n",
    "    for lang, count in topic_df_count.items():\n",
    "        if lang not in Languages.all:\n",
    "            continue\n",
    "        percentage = 100 * count / len(filtered_speeches[lang])\n",
    "        row_dict[f'{lang}_count'] = count\n",
    "        row_dict[f'{lang}_percentage'] = percentage\n",
    "    df_data.append(row_dict)\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T07:35:47.040672Z",
     "start_time": "2023-05-31T07:35:23.087855Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "for topic in tqdm(topics):\n",
    "    topic_df = topic_dfs[topic]\n",
    "    for language, language_speeches in filtered_speeches.items():\n",
    "        language_speeches[language_speeches.ID.isin(topic_df.common)].drop('Unnamed: 0', axis=1).to_csv(f'artefacts/by_topic/topic_csvs/{topic}_{language}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T09:04:32.203387Z",
     "start_time": "2023-05-30T09:03:45.191038Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:46<00:00, 11.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for language, df in tqdm(filtered_speeches.items()):\n",
    "    df.drop('Unnamed: 0', axis=1).to_csv(f'artefacts/bojan/filtered/{language}_filtered.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
