{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T12:47:39.845697Z",
     "start_time": "2023-05-31T12:47:39.783992Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy import spatial\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial import distance\n",
    "import heapq\n",
    "\n",
    "class Languages:\n",
    "    si = 'SI'\n",
    "    gb = 'GB'\n",
    "    hu = 'HU'\n",
    "    ua = 'UA'\n",
    "    all = [si, gb, hu, ua]\n",
    "\n",
    "\n",
    "plot_artefacts = Path('artefacts/plots')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:00:33.022636Z",
     "start_time": "2023-05-31T10:59:56.762122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_artefacts = Path('artefacts/by_topic/topic_csvs')\n",
    "embedding_artefacts_base = Path('artefacts/bojan')\n",
    "embedding_artefacts = {\n",
    "    Languages.si: pickle.load(open(embedding_artefacts_base / 'ParlaMint_SI_embeddings_truncated.pkl', 'rb')),\n",
    "    Languages.gb: pickle.load(open(embedding_artefacts_base / 'ParlaMint_GB_commons_embeddings_truncated.pkl', 'rb')),\n",
    "    Languages.ua: pickle.load(open(embedding_artefacts_base / 'ParlaMint_UA_embeddings_truncated.pkl', 'rb')),\n",
    "    Languages.hu: pickle.load(open(embedding_artefacts_base / 'ParlaMint_HU_embeddings_truncated.pkl', 'rb')),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:00:54.002431Z",
     "start_time": "2023-05-31T11:00:34.049586Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.80s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_names = ['war', 'eu', 'healthcare', 'gender']\n",
    "topics = {\n",
    "    t: {\n",
    "        l: pd.read_csv(topic_artefacts / f'{t}_{l}.csv') for l in tqdm(embedding_artefacts)\n",
    "    } for t in topic_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GPT Context\n",
    "Dataframes:\n",
    "- topic_names = ['war', 'eu', 'healthcare', 'gender']\n",
    "- topics[<topic_name>][Languages.<si/gb/ua/hu>] with columns:\n",
    "['ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Speaker_role', 'Speaker_MP', 'Speaker_Minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'speech', 'speech_split', 'sentiment', 'speech_length']\n",
    "- embedding_artefacts[Languages.<si/gb/ua/hu>]: dict of embeddings with keys = speech IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:32:00.376048Z",
     "start_time": "2023-05-31T10:32:00.315262Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Speaker_role', 'Speaker_MP', 'Speaker_Minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'speech', 'speech_split', 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "print(list(embedding_artefacts[Languages.si].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:04:23.040235Z",
     "start_time": "2023-05-31T11:04:22.978835Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Title', 'Date', 'Body', 'Term', 'Session', 'Meeting', 'Sitting', 'Agenda', 'Subcorpus', 'Speaker_role', 'Speaker_MP', 'Speaker_Minister', 'Speaker_party', 'Speaker_party_name', 'Party_status', 'Speaker_name', 'Speaker_gender', 'Speaker_birth', 'speech', 'speech_split', 'sentiment', 'speech_length']\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_for_speaker(embeddings, speech_ids):\n",
    "    return np.average([embeddings[sid] for sid in speech_ids if sid in embeddings], axis=0)\n",
    "\n",
    "\n",
    "print(list((topics['war'][Languages.si].columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T08:41:10.067071Z",
     "start_time": "2023-05-31T08:41:09.824945Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "894"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = Languages.gb\n",
    "topic_df = topics['war'][lang]\n",
    "\n",
    "\n",
    "def get_color_for_party(party):\n",
    "    if party == 'LAB':\n",
    "        return 'blue'\n",
    "    elif party == 'CON':\n",
    "        return 'red'\n",
    "    else:\n",
    "        return 'white'\n",
    "\n",
    "\n",
    "embeddings_subset = []\n",
    "colors = []\n",
    "for speaker_name, speaker_group in topic_df.groupby('Speaker_name'):\n",
    "    embeddings_subset.append(get_embeddings_for_speaker(embedding_artefacts[lang], speaker_group.ID.values))\n",
    "    sample = next(speaker_group.iterrows())[1]\n",
    "    colors.append(get_color_for_party(sample.Speaker_party))\n",
    "len(embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parties = []\n",
    "for party, subc in topics['war'][Languages.hu].groupby('Speaker_party'):\n",
    "    parties.append((party, len(subc)))\n",
    "for p in sorted(parties, key=lambda x: x[1], reverse=True):\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Polarization grid\n",
    "## Cos distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T13:01:19.305480Z",
     "start_time": "2023-05-31T13:01:19.244268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distance(ax, distance, min_distance, max_distance):\n",
    "    ax.barh([0], [distance], color='black', alpha=1)\n",
    "    # ax.barh([0], [1 - distance], left=[distance], color='#aaaaaa', alpha=1)\n",
    "    ax.set_xlim(min_distance, max_distance)\n",
    "    ax.set_yticks([])\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.set_xlabel(f'Distance = {distance:.2f}')\n",
    "\n",
    "def get_distance(embs, colors, top_n=20):\n",
    "    # Separate embeddings based on color\n",
    "    party1 = [embedding for embedding, color in zip(embs, colors) if color == c1]\n",
    "    party2 = [embedding for embedding, color in zip(embs, colors) if color == c2]\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    pairwise_distances = []\n",
    "    for embedding1 in party1:\n",
    "        for embedding2 in party2:\n",
    "            pairwise_distances.append(distance.cosine(embedding1, embedding2))\n",
    "\n",
    "    # Find the top N largest distances\n",
    "    top_distances = heapq.nlargest(top_n, pairwise_distances)\n",
    "    bottom_distances = heapq.nsmallest(top_n, pairwise_distances)\n",
    "\n",
    "    # Return the average of these distances\n",
    "    return np.median(top_distances) / np.median(bottom_distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-31T13:21:06.497760Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "c1 = 'purple'\n",
    "c2 = 'orange'\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "def get_data_for_topic(topic, lang, parties, embeddings_artefacts):\n",
    "    topic_df = topics[topic][lang]\n",
    "\n",
    "    # Only include data for specified parties\n",
    "    topic_df = topic_df[topic_df['Speaker_party'].isin(parties[0] + parties[1])]\n",
    "    embeddings_subset = []\n",
    "    colors = []\n",
    "    for speaker_name, speaker_group in topic_df.groupby('Speaker_name'):\n",
    "        embeddings_subset.append(get_embeddings_for_speaker(embeddings_artefacts[lang], speaker_group.ID.values))\n",
    "        sample = next(speaker_group.iterrows())[1]\n",
    "        if sample.Speaker_party in parties[0]:\n",
    "            colors.append(c1)\n",
    "        elif sample.Speaker_party in parties[1]:\n",
    "            colors.append(c2)\n",
    "        else:\n",
    "            colors.append('white')\n",
    "\n",
    "    return embeddings_subset, colors\n",
    "\n",
    "\n",
    "def apply_dimred(embeddings_subset):\n",
    "    # pca = PCA(n_components=2)\n",
    "    # reduced_embeddings = pca.fit_transform(embeddings_subset)\n",
    "    reducer = UMAP(n_neighbors=15,\n",
    "                   min_dist=0.2,\n",
    "                   n_components=2,\n",
    "                   metric='euclidean')\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings_subset)\n",
    "    return reduced_embeddings\n",
    "\n",
    "\n",
    "def plot_topic(ax, reduced_embeddings, colors, legend, lang, topic):\n",
    "    ax.scatter(\n",
    "        reduced_embeddings[:, 0],\n",
    "        reduced_embeddings[:, 1],\n",
    "        c=colors,\n",
    "        s=4,\n",
    "    )\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    ax.set_title(f'{lang} - {topic}')  # Add a title to each subplot\n",
    "\n",
    "    # Create legend\n",
    "    legend_elements = [Patch(facecolor=c, edgecolor='black', label=party) for party, c in legend]\n",
    "    ax.legend(handles=legend_elements, title='Parties', loc='upper right')\n",
    "\n",
    "\n",
    "def plot_languages(party_combinations, topic_names, embeddings_artefacts, plot_artefacts):\n",
    "    # get distances\n",
    "    distances = []\n",
    "    # Iterate over each language\n",
    "    for i, (lang, parties) in enumerate(party_combinations.items()):\n",
    "        # Iterate over each topic\n",
    "        for j, topic in enumerate(topic_names):\n",
    "            embeddings_subset, colors = get_data_for_topic(topic, lang, parties, embeddings_artefacts)\n",
    "            distances.append(get_distance(embeddings_subset, colors))\n",
    "\n",
    "    min_distance, max_distance = min(distances), max(distances)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        8, 4, figsize=(15, 16),\n",
    "        gridspec_kw={'height_ratios': [1, 0.05] * 4}\n",
    "    )  # Prepare a 8x4 grid of subplots\n",
    "\n",
    "    # Iterate over each language & do the plotting\n",
    "    for i, (lang, parties) in enumerate(party_combinations.items()):\n",
    "        # Iterate over each topic\n",
    "        for j, topic in enumerate(topic_names):\n",
    "            legend = [(', '.join([p.split('-')[0] for p in parties[0]]), c1),\n",
    "                      (', '.join([p.split('-')[0] for p in parties[1]]), c2)]\n",
    "\n",
    "            embeddings_subset, colors = get_data_for_topic(topic, lang, parties, embeddings_artefacts)\n",
    "            reduced_embeddings = apply_dimred(embeddings_subset)\n",
    "            plot_topic(axs[2 * i][j], reduced_embeddings, colors, legend, lang, topic)\n",
    "            plot_distance(axs[2 * i + 1][j], distances.pop(0), min_distance, max_distance)\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplot parameters to give specified padding\n",
    "    plt.savefig(plot_artefacts / 'polarization_grid.png', dpi=300)  # Save the figure\n",
    "    plt.show()  # Display the figure\n",
    "\n",
    "\n",
    "party_combinations = {\n",
    "    Languages.si: (('SDS', 'NSi', 'SLS', 'SNS'), ('SD', 'Levica', 'ZL')),\n",
    "    Languages.gb: (('CON',), ('LAB',)),\n",
    "    Languages.ua: (('фБПП', 'фЄС', 'фУДАР'), ('фОПЗЖ', 'фОпоблок', 'фПР')),\n",
    "    Languages.hu: (('Fidesz-frakció', 'JOBBIK-frakció', 'KNDP-frakció'), ('MSZP-frakció', 'LMP-frakció', 'DK-frakció')),\n",
    "}\n",
    "\n",
    "plot_languages(party_combinations, topic_names, embedding_artefacts, plot_artefacts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
